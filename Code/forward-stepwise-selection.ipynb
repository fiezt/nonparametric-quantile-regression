{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import multiprocessing\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "data_dir = curr_dir + '/../Data'\n",
    "solar_dir = data_dir + '/Solar'\n",
    "wind_dir = data_dir + '/Wind'\n",
    "price_dir = data_dir + '/Price'\n",
    "load_dir = data_dir + '/Load'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join('housing-data.csv'), header=None).values\n",
    "x = data[:, :-1]\n",
    "y = data[:, -1, None]\n",
    "train_idx = random.sample(range(len(x)), len(x)/2)\n",
    "test_idx = list(set(range(len(x))) - set(train_idx))\n",
    "x_train = x.take(train_idx, axis=0)\n",
    "x_test = x.take(test_idx, axis=0)\n",
    "y_train = y.take(train_idx, axis=0)\n",
    "y_test = y.take(test_idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(x_train, y_train, x_test, intercept=True):\n",
    "    \"\"\"Compute the least squares predictions.\n",
    "    \n",
    "    :param x_train: Numpy array like of the training feature space.\n",
    "    :param y_train: Numpy array like of the training labels.\n",
    "    :param x_test: Numpy array like of the feature space to predict on.\n",
    "    :param intercept: Boolean indicating whether or not an intercept has been\n",
    "    added to the feature spaces yet.\n",
    "    \n",
    "    :return predictions: Numpy array of the predictions for the training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    if intercept:\n",
    "        pass\n",
    "    else:\n",
    "        n_tr = len(x_train)\n",
    "        n_t = len(x_test)\n",
    "\n",
    "        x_train = np.hstack((np.ones((n_tr, 1)), x_train))\n",
    "        x_test = np.hstack((np.ones((x_t, 1)), x_test))\n",
    "    \n",
    "    beta = np.linalg.pinv(x_train.T.dot(x_train)).dot(x_train.T.dot(y_train))\n",
    "    \n",
    "    predictions = x_test.dot(beta)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstqr_hat_matrix(x_train, y_train, intercept=True):\n",
    "    \"\"\"Calculating the smoothing matrix S.\n",
    "    \n",
    "    :param x_train: Numpy array like of the training feature space.\n",
    "    :param y_train: Numpy array like of the training labels.\n",
    "    :param intercept: Boolean indicating whether or not an intercept has been\n",
    "    added to the feature spaces yet.\n",
    "    \n",
    "    :return hat_matrix: Numpy array of the smoothing matrix S for the model.\n",
    "    \"\"\"\n",
    "        \n",
    "    if intercept:\n",
    "        pass\n",
    "    else:\n",
    "        n_tr = len(x_train)\n",
    "        x_train = np.hstack((np.ones((n_tr, 1)), x_train))\n",
    "        \n",
    "    hat_matrix = x_train.dot(np.linalg.pinv(x_train.T.dot(x_train))).dot(x_train.T)\n",
    "    \n",
    "    return hat_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def efficient_LOOCV(x_train, y_train, learning_alg, smoothing_alg):\n",
    "    \"\"\"Compute the LOOCV for a training and testing set of data.\n",
    "    \n",
    "    :param x_train: Numpy array like of the feature space.\n",
    "    :param y_train: Numpy array like of the training labels.\n",
    "    :param learning_alg: Function to compute the predictions for the training \n",
    "    data. This learning algorithm must be a linear smoother.\n",
    "    :param smoothing_alg: Function to compute the smoothing matrix.\n",
    "    \n",
    "    :return loocv: The leave one out cross validation error.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = float(len(x_train))\n",
    "    \n",
    "    hat_matrix = smoothing_alg(x_train, y_train)\n",
    "    hat_diag = np.diagonal(hat_matrix).reshape((-1,1))\n",
    "    \n",
    "    f_hat = learning_alg(x_train, y_train, x_train)\n",
    "    \n",
    "    residuals = y_train - f_hat\n",
    "    \n",
    "    smoothing = 1 - hat_diag\n",
    "\n",
    "    loocv_vector = residuals/smoothing\n",
    "\n",
    "    loocv = 1/n * np.dot(loocv_vector.T, loocv_vector)\n",
    "        \n",
    "    return float(loocv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_stepwise_selection_lm(x, y, q, learning_alg, smoothing_alg):\n",
    "    \"\"\"Choose features to use by forward stepwise selection for a linear model.\n",
    "    \n",
    "    :param x: Numpy array like of the feature space.\n",
    "    :param y: Numpy array like of the labels for the data.\n",
    "    :param q: Integer of the number of features to select.\n",
    "    :param learning_alg: Function to compute the predictions for the training \n",
    "    data. This learning algorithm must be a linear smoother.\n",
    "    :param smoothing_alg: Function to compute the smoothing matrix.\n",
    "    \n",
    "    :return x_prime: Numpy array of the new feature space selected.\n",
    "    :return j_prime: List of the indexes of the feature space that were kept.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = x.shape[0]\n",
    "    p = x.shape[1]\n",
    "    \n",
    "    # Begin with an intercept term.    \n",
    "    x_prime = np.ones((n, 1))\n",
    "    j_prime = []\n",
    "    \n",
    "    for i in range(q):\n",
    "        \n",
    "        # Computing the loocv by adding each feature.\n",
    "        errors = [efficient_LOOCV(np.hstack((x_prime, x[:, j, None])), y, learning_alg, smoothing_alg) \n",
    "                  if j not in j_prime else float('inf') for j in xrange(p)] \n",
    "        \n",
    "        # Finding the feature index that minimized the loocv by adding the feature.\n",
    "        j_prime.append(np.argmin(np.array(errors)))\n",
    "\n",
    "        # Adding the best feature to the new feature vector to keep.\n",
    "        x_prime = np.hstack((x_prime, x[:, j_prime[-1], None]))\n",
    "    \n",
    "    x_prime = x_prime[:, 1:]\n",
    "    \n",
    "    return x_prime, j_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_means(x, k, num_restarts=15):\n",
    "    \"\"\"K-means++ algorithm for RBF feature creation. \n",
    "    \n",
    "    This function calls the k-means++ algorithm with multiple restarts \n",
    "    concurrently and chooses the start that minimized the loss.\n",
    "    \n",
    "    :param x: Feature space to create the RBF features from.\n",
    "    :param k: Integer number of clusters to use for the algorithm.\n",
    "    :param num_restarts: Integer number of times to restart the algorithm to \n",
    "    avoid local min. The iteration results that minimized the loss is chosen.\n",
    "    \n",
    "    :return loss: Final result for the loss function.\n",
    "    :return means: Numpy array with each row a centroid of a cluster. \n",
    "    :return bandwidths: Numpy array with each row the selected bandwidth for\n",
    "    for the corresponding cluster.\n",
    "    \"\"\"\n",
    "    \n",
    "    pool = multiprocessing.Pool()\n",
    "    \n",
    "    func = partial(k_means_alg, x, k)\n",
    "    results = pool.map(func, range(num_restarts))    \n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()    \n",
    "    \n",
    "    loss_results = np.array([result[0] for result in results])\n",
    "    best_loss_index = np.argmin(loss_results)\n",
    "    \n",
    "    loss, means, bandwidths = results[best_loss_index]\n",
    "    \n",
    "    return loss, means, bandwidths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_means_alg(x, k, restart_num):\n",
    "    \"\"\"K-means++ algorithm for RBF feature creation. \n",
    "    \n",
    "    This function implements the k-means++ algorithm, which is an approach to\n",
    "    sproad out the initial cluster centers by choosing a first cluster center at \n",
    "    random, then proceeding to choose centers sampled from the data with \n",
    "    probability proportional to the squared distance to the points existing \n",
    "    closest center.\n",
    "    \n",
    "    :param x: Feature space to create the RBF features from.\n",
    "    :param k: Integer number of clusters to use for the algorithm.\n",
    "    :param restart_num: Count of which restart is being called.\n",
    "    \n",
    "    :return loss: Final result for the loss function.\n",
    "    :return means: Numpy array with each row a centroid of a cluster. \n",
    "    :return bandwidths: Numpy array with each row the selected bandwidth for\n",
    "    for the corresponding cluster.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(x)\n",
    "    \n",
    "    # Seed numpy random number generator.\n",
    "    np.random.seed()\n",
    "    \n",
    "    # Choosing the first cluster centroid uniformly at random.\n",
    "    init = np.random.choice(range(n), 1, replace=False).tolist()\n",
    "    means = x[init]\n",
    "    \n",
    "    # Choosing the rest of the cluster centroid initializations.\n",
    "    for i in range(1,k):\n",
    "        \n",
    "        # Finding minimum squared distance for each point to an existing center.\n",
    "        squared_dists = np.array([[np.linalg.norm(x[j]-means[l])**2 for l in xrange(i)] for j in xrange(n)])\n",
    "        dist_mins = squared_dists.min(axis=1)\n",
    "        \n",
    "        # Sampling with probability proportional to the minimum squared distance.\n",
    "        prob_weights = dist_mins/dist_mins.sum()\n",
    "        sample = np.random.multinomial(1, prob_weights).tolist()\n",
    "        sample_choice = sample.index(1)\n",
    "        \n",
    "        means = np.vstack((means, x[sample_choice]))\n",
    "    \n",
    "    # K-means algorithm.\n",
    "    old_labels = np.nan * np.zeros(n)\n",
    "    converged = False\n",
    "    \n",
    "    while not converged:\n",
    "\n",
    "        # Assigning points to clusters.\n",
    "        squared_dists = [[np.linalg.norm(x[i]-means[l])**2 for l in xrange(k)] for i in xrange(n)]\n",
    "        labels = np.argmin(np.array(squared_dists), axis=1)\n",
    "                \n",
    "        # Check for convergence, given by no labels changing.\n",
    "        if np.array_equal(labels, old_labels):\n",
    "            converged = True\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        old_labels = labels\n",
    "            \n",
    "        # Recalculating the cluster centroids.\n",
    "        means = np.array([x[np.where(labels == i)[0]].mean(axis=0).tolist() for i in xrange(k)])    \n",
    "    \n",
    "    # Calculating the total loss.\n",
    "    squared_dists = [[np.linalg.norm(x[i]-means[l])**2 for l in xrange(k)] for i in xrange(n)]\n",
    "    loss = np.array(squared_dists).min(axis=1).sum()\n",
    "    \n",
    "    # Calculating the bandwidth for RBF features using median trick.\n",
    "    bandwidths = [[np.linalg.norm(means[i] - means[j]) for j in xrange(k) if i != j] for i in xrange(k)]\n",
    "    bandwidths = np.median(np.array(bandwidths), axis=1).reshape((-1,1))\n",
    "    \n",
    "    return loss, means, bandwidths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_prime, j_prime = forward_stepwise_selection_lm(x_train, y_train, 5, least_squares, lstqr_hat_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means(x_prime,5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
